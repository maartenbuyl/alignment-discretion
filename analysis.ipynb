{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:14.917126500Z",
     "start_time": "2025-02-09T20:52:14.507009600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import kendalltau, bootstrap\n",
    "from src.load_huggingface import load_huggingface as load_huggingface\n",
    "\n",
    "ROOT_DIR = os.path.abspath('.')\n",
    "FIGURES_DIR = os.path.join(ROOT_DIR, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a13b3b9329bfd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:16.594263300Z",
     "start_time": "2025-02-09T20:52:16.523263600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_name = \"Anthropic/hh-rlhf\"\n",
    "split = 'test'\n",
    "principle_pref_files = ['principle_specific_preferences/hh_test_seed_4ob.csv']\n",
    "main_a_pref = 'a_pref'\n",
    "\n",
    "# dataset_name = \"PKU-Alignment/PKU-SafeRLHF\"  # We will load -single-dimension later down\n",
    "# split = 'test'\n",
    "# principle_pref_files = ['principle_specific_preferences/pku_test_4ob.csv', 'principle_specific_preferences/pku_single_test_4ob.csv']\n",
    "# main_a_pref = 'a_pref_single'\n",
    "\n",
    "\n",
    "# We use short descriptions of principles here for visualization purposes.\n",
    "# See 'principles/ccai_seed.txt' for actual texts.\n",
    "principles_file = 'principles/ccai_seed_short.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05039a03f77440",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998bd7d6c2037ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574984be73558b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:26.578620500Z",
     "start_time": "2025-02-09T20:52:18.788914900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset, principles = load_huggingface(dataset_name, principles_name=principles_file, split=split)\n",
    "dataset['row_id'] = dataset['row_id'].astype(str)\n",
    "dataset = dataset.set_index('row_id')\n",
    "dataset = dataset.rename(columns={'prompt': 'x', 'rejected': 'y_0', 'chosen': 'y_1'})\n",
    "data_suff = dataset_name.split('/')[-1]\n",
    "\n",
    "if dataset_name == \"PKU-Alignment/PKU-SafeRLHF\":\n",
    "    single_dim_dataset = load_huggingface(\"PKU-Alignment/PKU-SafeRLHF-single-dimension\", split=split)[0]\n",
    "    single_dim_dataset['row_id'] = single_dim_dataset['row_id'].astype(str) + '_single'\n",
    "    single_dim_dataset = single_dim_dataset.rename(columns={'prompt': 'x', 'rejected': 'y_0', 'chosen': 'y_1'})\n",
    "    single_dim_dataset = single_dim_dataset.set_index('row_id')\n",
    "    dataset = pd.concat([dataset, single_dim_dataset], axis=0)\n",
    "print(f\"Loaded {len(dataset)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fb4981d7a01f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:26.578620500Z",
     "start_time": "2025-02-09T20:52:26.545619800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcf576c1abcf53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:26.653628100Z",
     "start_time": "2025-02-09T20:52:26.573619800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "principles.name = 'principle'\n",
    "principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f24a4df713e549",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load principle prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e6ff618364db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:27.264631900Z",
     "start_time": "2025-02-09T20:52:26.597620600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_prefs_raw = []\n",
    "for f in principle_pref_files:\n",
    "    df = pd.read_csv(f).drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    df['row_id'] = df['row_id'].astype(str)\n",
    "    \n",
    "    if 'principle_id' in df and 'prob_A' in df:\n",
    "        for principle_id in df['principle_id'].unique():\n",
    "            df[f'prob_A_p{principle_id}'] = df[df['principle_id'] == principle_id]['prob_A']\n",
    "            df[f'prob_B_p{principle_id}'] = df[df['principle_id'] == principle_id]['prob_B']\n",
    "            if 'prob_NA' in df:\n",
    "                df[f'prob_NA_p{principle_id}'] = df[df['principle_id'] == principle_id]['prob_NA']\n",
    "        df = df.drop(columns=['prob_A', 'prob_B', 'prob_NA', 'principle_id'])  \n",
    "    if dataset_name == \"PKU-Alignment/PKU-SafeRLHF\" and 'single' in f:\n",
    "        df['row_id'] = df['row_id'] + '_single'\n",
    "    df['file'] = f\n",
    "    \n",
    "    # Remapping for old data formats\n",
    "    if 'order' in df:\n",
    "        df['order'] = df['order'].replace({'0,1': '0', '1,0': '1'}).astype(int)\n",
    "    df = df.rename(columns={col: col.replace('prob_', 'score_') for col in df.columns})\n",
    "    df = df.rename(columns={col: col.replace('_A_', '_1_') for col in df.columns})\n",
    "    df = df.rename(columns={col: col.replace('_B_', '_0_') for col in df.columns})\n",
    "    df = df.rename(columns={col: col.replace('_na_', '_NA_') for col in df.columns})\n",
    "    \n",
    "    score_cols = [col for col in df.columns if col.startswith('score_')]\n",
    "    score_cols += ['cost'] if 'cost' in df else []\n",
    "    df = df.groupby(['file', 'row_id', 'order'])[score_cols].sum().reset_index()\n",
    "    df = df.set_index(['file', 'row_id'])\n",
    "    \n",
    "    p_prefs_raw.append(df)\n",
    "p_prefs_raw = pd.concat(p_prefs_raw)\n",
    "\n",
    "# For debugging purposes, only keep first order\n",
    "p_prefs_raw = p_prefs_raw[p_prefs_raw['order'] == 0]\n",
    "if p_prefs_raw['order'].nunique() == 1:\n",
    "    p_prefs_raw = p_prefs_raw.drop(columns=['order'])\n",
    "\n",
    "p_prefs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa6ad16bb82293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:27.265632Z",
     "start_time": "2025-02-09T20:52:26.976625300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pref(df, col_a='score_1', col_b='score_0', col_na='score_NA', discrete=True):\n",
    "    if not discrete:\n",
    "        raise NotImplementedError\n",
    "    a_vals = df[col_a]\n",
    "    b_vals = df[col_b]\n",
    "    a_over_b = np.sign(a_vals - b_vals)\n",
    "    a_over_b[a_over_b == 0] = np.nan\n",
    "    if 'order' in df:\n",
    "        b_over_a_reversed = -a_over_b[df['order'] == 1]\n",
    "        combined_a_over_b = (a_over_b[df['order'] == 0] + b_over_a_reversed) / 2\n",
    "        a_over_b = combined_a_over_b\n",
    "    \n",
    "    if col_na is not None:\n",
    "        na_vals = df[col_na]\n",
    "        if 'order' in df:\n",
    "            a_over_b.loc[((na_vals > a_vals) & (na_vals > b_vals))[df['order'] == 0]] = np.nan\n",
    "            a_over_b.loc[((na_vals > a_vals) & (na_vals > b_vals))[df['order'] == 1]] = np.nan\n",
    "        else:\n",
    "            a_over_b.loc[(na_vals > a_vals) & (na_vals > b_vals)] = np.nan\n",
    "    return a_over_b\n",
    "\n",
    "def get_pref_per_principle(df):\n",
    "    score_a_cols = [col for col in df.columns if f'score_1' in col]\n",
    "    score_b_cols = [col for col in df.columns if f'score_0' in col]\n",
    "    score_na_cols = [col for col in df.columns if f'score_NA' in col]\n",
    "    assert len(score_a_cols) == len(score_b_cols) == len(principles)\n",
    "    if len(score_na_cols) == 0:\n",
    "        score_na_cols = None\n",
    "    else:\n",
    "        assert len(score_na_cols) == len(score_a_cols)\n",
    "    \n",
    "    a_over_b_per_principle = {}\n",
    "    for p_i in range(len(principles)):\n",
    "        p_pref = get_pref(df,\n",
    "                          col_a=score_a_cols[p_i], \n",
    "                          col_b=score_b_cols[p_i], \n",
    "                          col_na=score_na_cols[p_i])\n",
    "        a_over_b_per_principle[f'p{p_i}_pref'] = p_pref\n",
    "    a_over_b_df = pd.concat(a_over_b_per_principle, axis=1)\n",
    "    return a_over_b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3e529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:27.265632Z",
     "start_time": "2025-02-09T20:52:26.986619800Z"
    }
   },
   "outputs": [],
   "source": [
    "p_prefs = get_pref_per_principle(p_prefs_raw)\n",
    "p_prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817dd9a0cbb2cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:27.266635400Z",
     "start_time": "2025-02-09T20:52:27.035620200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select only one oracle file\n",
    "if dataset_name == \"Anthropic/hh-rlhf\":\n",
    "    p_prefs = p_prefs.loc[principle_pref_files[0]]\n",
    "elif dataset_name == \"PKU-Alignment/PKU-SafeRLHF\":\n",
    "    p_prefs_dual = p_prefs.loc[principle_pref_files[0]]\n",
    "    p_prefs_single = p_prefs.loc[principle_pref_files[1]]\n",
    "    p_prefs = pd.concat([p_prefs_dual, p_prefs_single], axis=0)\n",
    "p_prefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ebb5266c424da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load reward model prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90911317a84ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:27.266635400Z",
     "start_time": "2025-02-09T20:52:27.068620100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from reward_model_preferences.files import file_dict as all_rm_pref_files\n",
    "rm_pref_files = all_rm_pref_files[dataset_name].copy()\n",
    "if dataset_name == \"PKU-Alignment/PKU-SafeRLHF\":\n",
    "    rm_pref_files.update(all_rm_pref_files[\"PKU-Alignment/PKU-SafeRLHF-single-dimension\"])\n",
    "\n",
    "rm_prefs_raw = []\n",
    "for f, f_short in rm_pref_files.items():\n",
    "    df = pd.read_csv(f).drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    df['row_id'] = df['row_id'].astype(str)\n",
    "    df['file'] = f_short\n",
    "    if f in all_rm_pref_files[\"PKU-Alignment/PKU-SafeRLHF-single-dimension\"]:\n",
    "        df['row_id'] = df['row_id'] + '_single'\n",
    "    df = df.set_index('row_id')\n",
    "    # df = df.rename(columns={col: col.replace('reward_', 'score_') for col in df.columns})\n",
    "    \n",
    "    better_col = None\n",
    "    if dataset_name == \"Anthropic/hh-rlhf\":\n",
    "        better_col = 'a_pref'\n",
    "    elif dataset_name == \"PKU-Alignment/PKU-SafeRLHF\":\n",
    "        if 'splited' in f:\n",
    "            better_col = 'a_pref_better'\n",
    "        else:\n",
    "            better_col = 'a_pref_single'\n",
    "    # _a is coded as the preferred response according to 'better_col'.\n",
    "    # To find the score of response_1, we thus check if 1 is the preferred response.\n",
    "    better_rows = dataset.loc[dataset.index.isin(df.index), better_col] == 1\n",
    "    df.loc[better_rows, 'score_1'] = df.loc[better_rows, 'reward_a']\n",
    "    df.loc[better_rows, 'score_0'] = df.loc[better_rows, 'reward_b']\n",
    "    \n",
    "    # If 0 is the preferred response, we need to swap the scores\n",
    "    df.loc[~better_rows, 'score_1'] = df.loc[~better_rows, 'reward_b']\n",
    "    df.loc[~better_rows, 'score_0'] = df.loc[~better_rows, 'reward_a']\n",
    "\n",
    "    df = df.reset_index().set_index(['file', 'row_id'])\n",
    "    rm_prefs_raw.append(df[['score_1', 'score_0']])\n",
    "rm_prefs_raw = pd.concat(rm_prefs_raw)\n",
    "rm_prefs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0a3bae16e3eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:27.268627100Z",
     "start_time": "2025-02-09T20:52:27.131620Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rm_prefs = get_pref(rm_prefs_raw, col_na=('score_NA' if 'score_NA' in rm_prefs_raw else None))\n",
    "rm_prefs.name = 'rm_'\n",
    "rm_prefs = rm_prefs.reset_index().pivot(index='row_id', columns='file', values='rm_').rename_axis(None, axis=1)\n",
    "rm_prefs = rm_prefs.rename(columns={col: 'rm_' + col for col in rm_prefs.columns})\n",
    "rm_prefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbcbf41581c206",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load LLM prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3fc76ff441f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:28.627133300Z",
     "start_time": "2025-02-09T20:52:27.935054100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llm_preferences.files import file_dict as all_llm_pref_files\n",
    "llm_pref_files = all_llm_pref_files[dataset_name].copy()\n",
    "if dataset_name == \"PKU-Alignment/PKU-SafeRLHF\":\n",
    "    llm_pref_files.update(all_llm_pref_files[\"PKU-Alignment/PKU-SafeRLHF-single-dimension\"])\n",
    "\n",
    "llm_prefs = []\n",
    "for f, f_short in llm_pref_files.items():\n",
    "    df = pd.read_csv(f).drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    df['row_id'] = df['row_id'].astype(str)\n",
    "    df['file'] = f_short\n",
    "    if f in all_llm_pref_files[\"PKU-Alignment/PKU-SafeRLHF-single-dimension\"]:\n",
    "        df['row_id'] = df['row_id'] + '_single'\n",
    "    df = df.set_index('row_id')\n",
    "    \n",
    "    df['order'] = df['order'].replace({'0,1': '0', '1,0': '1'}).astype(int)\n",
    "    \n",
    "    # df = df.rename(columns={col: col.replace('reward_', 'score_') for col in df.columns})\n",
    "    # df = df.drop(columns=['log_prob_chat_chosen', 'log_prob_chat_rejected'], errors='ignore')\n",
    "    \n",
    "    if 'log_prob_chat_chosen' in df:\n",
    "        df = df.rename(columns={\n",
    "            'log_prob_chat_chosen': 'score_a', \n",
    "            'log_prob_chat_rejected': 'score_b'}\n",
    "        )\n",
    "        df = df.drop(columns=['reward_a', 'reward_b', 'reward_NA'], errors='ignore')\n",
    "        assert (df.groupby(['row_id'])['score_a'].std() == 0.0).all()\n",
    "        df = df[df['order'] == 0]\n",
    "        df = df.drop(columns=['order'])\n",
    "    \n",
    "    if 'score_a' in df:\n",
    "        better_col = None\n",
    "        if dataset_name == \"Anthropic/hh-rlhf\":\n",
    "            better_col = 'a_pref'\n",
    "        elif dataset_name == \"PKU-Alignment/PKU-SafeRLHF\":\n",
    "            if 'splited' in f:\n",
    "                better_col = 'a_pref_better'\n",
    "            else:\n",
    "                better_col = 'a_pref_single'\n",
    "        # _a is coded as the preferred response according to 'better_col'.\n",
    "        # To find the score of response_1, we thus check if 1 is the preferred response.\n",
    "        better_rows = dataset.loc[dataset.index.isin(df.index), better_col] == 1\n",
    "        df.loc[better_rows, 'score_1'] = df.loc[better_rows, 'score_a']\n",
    "        df.loc[better_rows, 'score_0'] = df.loc[better_rows, 'score_b']\n",
    "        \n",
    "        # If 0 is the preferred response, we need to swap the scores\n",
    "        df.loc[~better_rows, 'score_1'] = df.loc[~better_rows, 'score_b']\n",
    "        df.loc[~better_rows, 'score_0'] = df.loc[~better_rows, 'score_a']\n",
    "        \n",
    "        df = df.drop(columns=['score_a', 'score_b'])\n",
    "    \n",
    "    if 'generic' in f:\n",
    "        # For GPT-4o preferences, _A is already always response _1\n",
    "        df = df.rename(columns={col: col.replace('prob_', 'score_') for col in df.columns})\n",
    "        df = df.rename(columns={col: col.replace('_A', '_1') for col in df.columns})\n",
    "        df = df.rename(columns={col: col.replace('_B', '_0') for col in df.columns})\n",
    "        df = df.drop(columns=['cost'], errors='ignore')\n",
    "    \n",
    "    df = df.reset_index().set_index(['file', 'row_id'])\n",
    "    _prefs = get_pref(df, col_na=('score_NA' if 'score_NA' in df else None))\n",
    "    llm_prefs.append(_prefs)\n",
    "llm_prefs = pd.concat(llm_prefs)\n",
    "llm_prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f819368b029c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:29.703210800Z",
     "start_time": "2025-02-09T20:52:29.031133Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm_prefs.name = 'llm_'\n",
    "llm_prefs = llm_prefs.reset_index().pivot(index='row_id', columns='file', values='llm_').rename_axis(None, axis=1)\n",
    "llm_prefs = llm_prefs.rename(columns={col: 'llm_' + col for col in llm_prefs.columns})\n",
    "llm_prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672deba8013dc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:31.364638100Z",
     "start_time": "2025-02-09T20:52:31.329664900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = list(llm_prefs.columns)\n",
    "columns.insert(2, columns.pop(columns.index('llm_Sonnet3.5')))\n",
    "llm_prefs = llm_prefs[columns]\n",
    "llm_prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c290d50daf4a763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:32.125339600Z",
     "start_time": "2025-02-09T20:52:32.029089600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm_prefs.apply(lambda x: x.value_counts(normalize=True, dropna=False)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485f79791219573b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Principle-specific preferences analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65f599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:35.465373100Z",
     "start_time": "2025-02-09T20:52:35.055768200Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_stacked_counts(p_prefs):\n",
    "    na_counts = p_prefs.isna().mean(axis=0)\n",
    "    one_counts = (p_prefs == 1).mean(axis=0)\n",
    "    neg_one_counts = (p_prefs == -1).mean(axis=0)\n",
    "\n",
    "    counts_df = pd.DataFrame({\n",
    "        'Indifference': na_counts,\n",
    "        'Agreement': one_counts,\n",
    "        'Disagreement': neg_one_counts\n",
    "    })\n",
    "    \n",
    "    return counts_df\n",
    "stacked_counts_df = compute_stacked_counts(p_prefs)\n",
    "y = np.arange(len(stacked_counts_df)) * 1.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "ax.barh(y, stacked_counts_df['Indifference'], color='#C0C0C0', label='Indifference', height=0.6, edgecolor='black')\n",
    "ax.barh(y, stacked_counts_df['Agreement'], left=stacked_counts_df['Indifference'], color='#4682B4', label='Agreement', height=0.6, edgecolor='black')\n",
    "ax.barh(y, stacked_counts_df['Disagreement'], left=stacked_counts_df['Indifference'] + stacked_counts_df['Agreement'], color='#CD5C5C', label='Disagreement', height=0.6, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Proportion')\n",
    "ax.set_ylabel('Principle')\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(principles)\n",
    "ax.legend(\n",
    "    title='Category',\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.02),\n",
    "    frameon=True,\n",
    "    ncol=3\n",
    ")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, f'{data_suff}1.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768829d9a67d9f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:36.992689100Z",
     "start_time": "2025-02-09T20:52:36.769506800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def identify_p_agreement(p_prefs_row):\n",
    "    if p_prefs_row.isna().all():\n",
    "        return 'Indifference'\n",
    "    elif np.isclose(p_prefs_row, -1).any() and np.isclose(p_prefs_row, 1).any():\n",
    "        return 'Conflict'\n",
    "    else:\n",
    "        return 'Consensus'\n",
    "        \n",
    "p_agreement = pd.get_dummies(p_prefs.apply(identify_p_agreement, axis=1))\n",
    "categories = ['Indifference', 'Conflict', 'Consensus']\n",
    "for col in categories:\n",
    "    if col not in p_agreement:\n",
    "        p_agreement[col] = False\n",
    "assert (p_agreement[categories].sum(axis=1) == 1).all()\n",
    "p_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1693ba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:50.878625800Z",
     "start_time": "2025-02-09T20:52:50.664708Z"
    }
   },
   "outputs": [],
   "source": [
    "freqs = (\n",
    "    pd.get_dummies(p_agreement)\n",
    "    .reset_index()\n",
    "    .groupby('row_id')[categories]\n",
    "    .sum()\n",
    "    .apply(lambda x: x / x.sum(), axis=1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "freqs_stats_df = (\n",
    "    pd.DataFrame({\n",
    "        \"Category\": categories,\n",
    "        \"Mean (%)\": [freqs[cat].mean() * 100 for cat in categories],\n",
    "        \"SD (%)\": [freqs[cat].std()/len(freqs[cat])**(0.5) * 100 for cat in categories]\n",
    "    })\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "freqs_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd54d2bfa5ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:52:51.621931800Z",
     "start_time": "2025-02-09T20:52:51.567521200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefs = dataset.merge(rm_prefs, how='inner', left_index=True, right_index=True)\n",
    "prefs = prefs.merge(llm_prefs, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "prefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e309339136d037",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Discretion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e48bcd105140e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:53:00.558931900Z",
     "start_time": "2025-02-09T20:53:00.527911300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotator_pref_cols = [col for col in prefs.columns if col.startswith('a_pref')]\n",
    "annotator_pref_cols += [col for col in prefs.columns if col.startswith('rm_')]\n",
    "annotator_pref_cols += [col for col in prefs.columns if col.startswith('llm_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206f7c0bf564c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:53:01.603701400Z",
     "start_time": "2025-02-09T20:53:01.511304Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consensus_idx = p_agreement['Consensus']\n",
    "consensus = p_prefs[consensus_idx].min(axis=1)\n",
    "assert (consensus == p_prefs[consensus_idx].max(axis=1)).all()\n",
    "def compute_discretion_abuse(a_prefs_col):\n",
    "    agreement = (consensus * a_prefs_col) < 0\n",
    "    agreement = agreement[~a_prefs_col.isna() & (a_prefs_col != 0)]\n",
    "    return agreement.mean()\n",
    "discretion_abuse = prefs.loc[consensus_idx, annotator_pref_cols].apply(compute_discretion_abuse)\n",
    "discretion_abuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18e033",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def compute_discretion_abuse(a_prefs_col, consensus):\n",
    "    agreement = (consensus * a_prefs_col) < 0\n",
    "    agreement = agreement[~a_prefs_col.isna() & (a_prefs_col != 0)]\n",
    "    return agreement.mean()\n",
    "\n",
    "def bootstrap_discretion_abuse(n_bootstrap):\n",
    "    column_bootstrap_results = {col: [] for col in annotator_pref_cols}\n",
    "    valid_indices = consensus_idx[consensus_idx].index\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        sampled_indices = np.random.choice(valid_indices, size=len(valid_indices), replace=True)\n",
    "        sampled_prefs = prefs.loc[sampled_indices, annotator_pref_cols]\n",
    "        sampled_consensus = consensus.loc[sampled_indices]\n",
    "\n",
    "        for col in annotator_pref_cols:\n",
    "            discretion_abuse = compute_discretion_abuse(sampled_prefs[col], sampled_consensus)\n",
    "            column_bootstrap_results[col].append(discretion_abuse)\n",
    "\n",
    "    column_results = {}\n",
    "    for col, results in column_bootstrap_results.items():\n",
    "        if results:\n",
    "            lower, upper = np.percentile(results, [2.5, 97.5])\n",
    "            mean_value = np.mean(results)\n",
    "            largest_gap = max(abs(mean_value - lower), abs(mean_value - upper))\n",
    "            sd_value = np.std(results)\n",
    "            column_results[col] = {\n",
    "                'mean': mean_value,\n",
    "                'ci': (lower, upper),\n",
    "                'largest_gap': largest_gap,\n",
    "                'sd': sd_value\n",
    "            }\n",
    "        else:\n",
    "            column_results[col] = {\n",
    "                'mean': None,\n",
    "                'ci': (None, None),\n",
    "                'largest_gap': None,\n",
    "                'sd': None\n",
    "            }\n",
    "\n",
    "    return column_results\n",
    "\n",
    "discretion_abuse_results = bootstrap_discretion_abuse(n_bootstrap=1000)\n",
    "\n",
    "discretion_abuse_stats = {\n",
    "    col: {\n",
    "        'Mean (%)': result['mean'] * 100 if result['mean'] is not None else None,\n",
    "        'SD (%)': result['sd'] * 100 if result['sd'] is not None else None,\n",
    "        # 'CI Lower (%)': result['ci'][0] * 100 if result['ci'] else None,\n",
    "        # 'CI Upper (%)': result['ci'][1] * 100 if result['ci'] else None,\n",
    "        # 'Largest Gap (%)': result['largest_gap'] * 100 if result['largest_gap'] else None\n",
    "    }\n",
    "    for col, result in discretion_abuse_results.items()\n",
    "    if result['mean'] is not None\n",
    "}\n",
    "\n",
    "discretion_abuse_stats = pd.DataFrame.from_dict(discretion_abuse_stats, orient='index')\n",
    "discretion_abuse_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a5e08d3712ed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:54:56.499515800Z",
     "start_time": "2025-02-09T20:54:44.989368300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncols = len(p_prefs.columns)\n",
    "\n",
    "if data_suff == 'PKU-SafeRLHF':\n",
    "    labs = [\"human\", \"better\" , \"safer\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (base)\",\"LlaMa-8B (finetuned)\", \"Mistral-7B (base)\", \"Mistral-7B (finetuned)\"]\n",
    "elif data_suff == \"hh-rlhf\":\n",
    "    labs = [\"human\",\"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (base)\",\"LlaMa-8B (finetuned)\", \"Mistral-7B (base)\", \"Mistral-7B (finetuned)\"]\n",
    "else:\n",
    "    labs = []\n",
    "\n",
    "def compute_supremacies(a_prefs_col):\n",
    "    ncols = p_prefs.shape[1]\n",
    "    nb_conflicts = np.zeros((ncols, ncols))\n",
    "    supremacies = np.empty((ncols, ncols))\n",
    "    supremacies.fill(np.nan)\n",
    "    \n",
    "    a_prefs_valid = (~a_prefs_col.isna()) & (a_prefs_col != 0)\n",
    "    for i in range(ncols):\n",
    "        for j in range(ncols):\n",
    "            pi_prefs = p_prefs.loc[a_prefs_col.index].iloc[:, i]\n",
    "            pj_prefs = p_prefs.loc[a_prefs_col.index].iloc[:, j]\n",
    "            ij_conflict = (pi_prefs * pj_prefs) < 0\n",
    "            ij_conflict_valid = ij_conflict & a_prefs_valid\n",
    "            nb_conflicts[i, j] = ij_conflict_valid.sum()\n",
    "            supremacies[i, j] = ((pi_prefs[ij_conflict_valid] * a_prefs_col[ij_conflict_valid]) > 0).mean()\n",
    "    \n",
    "    supremacies = pd.DataFrame(supremacies, index=principles, columns=principles)\n",
    "    return supremacies, nb_conflicts\n",
    "\n",
    "supremacies_per_a = {}\n",
    "nb_conflicts_per_a = {}\n",
    "for k, a in enumerate(annotator_pref_cols):\n",
    "    supremacies_per_a[a], nb_conflicts_per_a[a] = compute_supremacies(prefs[a])\n",
    "    upper_triangular_sum = np.sum(np.triu(np.nan_to_num(nb_conflicts_per_a[a], nan=0.0), k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753d454f404939b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:54:57.737786Z",
     "start_time": "2025-02-09T20:54:56.509603200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ELOs / Importances\n",
    "import torch\n",
    "np.random.seed(5) \n",
    "def compute_elo(a_nb_conflicts, a_supremacies, nb_iter=30000):\n",
    "    ngames = torch.tensor(a_nb_conflicts, dtype=torch.float32)\n",
    "    y = torch.tensor(a_supremacies, dtype=torch.float32)\n",
    "    elo = torch.zeros((a_nb_conflicts.shape[:-1]), requires_grad=True)\n",
    "    \n",
    "    always_winners = torch.logical_or(y == 1, y.isnan()).all(dim=-1)\n",
    "    always_losers = torch.logical_or(y == 0, y.isnan()).all(dim=-1)\n",
    "    elo.data[always_winners] = torch.inf\n",
    "    elo.data[always_losers] = -torch.inf\n",
    "    \n",
    "    elos_req_grad = ~(always_winners | always_losers)\n",
    "    \n",
    "    assert elos_req_grad.any(dim=-1).all()\n",
    "    \n",
    "    # For fixing translation invariance. Doesnt work for vectorized ELO (in bootstrapping)\n",
    "    # first_valid_elo = torch.arange(elo.shape[-1])[elos_req_grad][0]\n",
    "    # elos_req_grad[first_valid_elo] = False\n",
    "    \n",
    "    y_flat = y.flatten(start_dim=-2)\n",
    "    y_flat = y_flat.nan_to_num(nan=0.5)\n",
    "    ngames_flat = ngames.flatten(start_dim=-2)\n",
    "    ngames_flat = ngames_flat.nan_to_num(nan=0)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([elo], lr=.1)\n",
    "    for i in range(nb_iter):        \n",
    "        optimizer.zero_grad()\n",
    "        predicted = elo.unsqueeze(-1) - elo.unsqueeze(-2)\n",
    "        predicted = predicted.flatten(start_dim=-2)\n",
    "        y_flat_finite = y_flat[predicted.isfinite()]\n",
    "        ngames_flat_finite = ngames_flat[predicted.isfinite()]\n",
    "        predicted = predicted[predicted.isfinite()]\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(predicted, y_flat_finite, weight=ngames_flat_finite)\n",
    "        # loss = loss + 0.001 * torch.sum(elo.pow(2))\n",
    "        loss.backward()\n",
    "        elo.grad[~elos_req_grad] = 0\n",
    "        if elo.grad.abs().max() < 1e-7:\n",
    "            print(f'Converged early after {i} iterations')\n",
    "            break\n",
    "        optimizer.step()\n",
    "        # losses.append(loss.item())\n",
    "        # if i % 100 == 0 and i > 0:\n",
    "        #     print(loss.item())\n",
    "    if i == nb_iter - 1:\n",
    "        print(f'Did not converge after {nb_iter} iterations')\n",
    "    return elo.detach().numpy()\n",
    "\n",
    "elos = {}\n",
    "for a in annotator_pref_cols:\n",
    "    elos[a] = compute_elo(nb_conflicts_per_a[a], supremacies_per_a[a].values)\n",
    "elos = pd.DataFrame(elos, index=principles)\n",
    "elos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526a579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:54:57.793485200Z",
     "start_time": "2025-02-09T20:54:57.734710300Z"
    }
   },
   "outputs": [],
   "source": [
    "elos = elos.sort_values(by=elos.columns[0], ascending=False)\n",
    "elos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13588785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:55:34.549688300Z",
     "start_time": "2025-02-09T20:55:21.002087300Z"
    }
   },
   "outputs": [],
   "source": [
    "principle_to_column = dict(zip(principles, p_prefs.columns))\n",
    "principle_to_i = dict(zip(principles, range(len(principles))))\n",
    "\n",
    "if data_suff == 'PKU-SafeRLHF':\n",
    "    labs = [\"human\", \"helpfulness\" , \"safety\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (base)\",\"LlaMa-8B (finetuned)\", \"Mistral-7B (base)\", \"Mistral-7B (finetuned)\"]\n",
    "elif data_suff == \"hh-rlhf\":\n",
    "    labs = [\"human\",\"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (base)\",\"LlaMa-8B (finetuned)\", \"Mistral-7B (base)\", \"Mistral-7B (finetuned)\"]\n",
    "else:\n",
    "    labs = []\n",
    "\n",
    "\n",
    "def compute_supremacies_for_selected(supremacies_per_a, nb_conflicts, selected_principles, k, show_plot=True,\n",
    "                                     order=None):\n",
    "     if order is None:\n",
    "         order = elos.iloc[selected_principles].index\n",
    "     selected_supremacies = supremacies_per_a.loc[order, order]\n",
    "     i_locs = [principle_to_i[p] for p in order]\n",
    "     selected_conflicts = nb_conflicts[np.ix_(i_locs, i_locs)]\n",
    "     if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        formatted_text = (np.array([\n",
    "            f\"{'/' if np.isnan(w) else f'{w*100:.0f}'}%\\n({int(n) if not np.isnan(n) else '/'})\"\n",
    "            for w, n in zip(selected_supremacies.to_numpy().flatten(), selected_conflicts.flatten())\n",
    "        ]).reshape(selected_supremacies.shape))\n",
    "        sns.heatmap(\n",
    "            selected_supremacies,\n",
    "            annot=formatted_text,\n",
    "            cmap=\"RdBu_r\",\n",
    "            fmt='', \n",
    "            cbar=True,\n",
    "            linewidths=0.5,\n",
    "            linecolor='gray',\n",
    "            square=True,\n",
    "            xticklabels=selected_supremacies.index,\n",
    "            yticklabels=selected_supremacies.index,\n",
    "            annot_kws={\"fontsize\": 7, \"fontweight\": \"bold\"}, \n",
    "            cbar_kws={'label': 'Supremacy', 'shrink': 0.8},\n",
    "            vmin=0, \n",
    "            vmax=1,\n",
    "            ax=ax\n",
    "        )\n",
    "        for edge, spine in ax.spines.items():\n",
    "            spine.set_visible(True) \n",
    "            spine.set_color('black')\n",
    "            spine.set_linewidth(0.5) \n",
    "        cbar = ax.collections[0].colorbar \n",
    "        cbar.set_ticks(cbar.get_ticks()) \n",
    "        cbar.set_ticklabels([f'{val * 100:.0f}' for val in cbar.get_ticks()]) \n",
    "        ax.set_xlabel(f'Principle that disagrees with {labs[k]}', fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_ylabel(f'Principle that agrees with {labs[k]}', fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'supr_{data_suff}_{k}.pdf'), transparent=True)\n",
    "        plt.show()\n",
    "\n",
    "k = 0\n",
    "for a in annotator_pref_cols:\n",
    "    if a.startswith('a_pref') | a.startswith('llm_'):\n",
    "        order = elos[a].sort_values(ascending=False).index\n",
    "        compute_supremacies_for_selected(supremacies_per_a[a], nb_conflicts_per_a[a], [i for i in range(21)], k, show_plot=True, order=order)\n",
    "        matrix = nb_conflicts_per_a[a]\n",
    "        matrix_cleaned = np.nan_to_num(matrix, nan=0.0)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3995647abd62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:55:34.757715200Z",
     "start_time": "2025-02-09T20:55:34.576882700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elo_ranks = elos.rank(ascending=False).astype(int).sort_values(main_a_pref)\n",
    "elo_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c6a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:55:34.758512400Z",
     "start_time": "2025-02-09T20:55:34.592063700Z"
    }
   },
   "outputs": [],
   "source": [
    "elos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc154fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:55:59.009729500Z",
     "start_time": "2025-02-09T20:55:56.323607400Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "blue_to_grey = LinearSegmentedColormap.from_list(\n",
    "    \"GreyBlue\",\n",
    "    [\n",
    "        (0.0, \"#D3D3D3\"),\n",
    "        (0.2, \"#A9A9A9\"),\n",
    "        (0.4, \"#808080\"),\n",
    "        (0.6, \"#6495ED\"),\n",
    "        (0.8, \"#4169E1\"),\n",
    "        (1.0, \"#0000A0\"),\n",
    "    ]\n",
    ")\n",
    "cmap = blue_to_grey\n",
    "norm = mcolors.Normalize(vmin=0.1, vmax=1)\n",
    "\n",
    "elos_filtered = elos.loc[:, ~elos.columns.str.contains(r'(base|w/o)', case=False)]\n",
    "elos_filtered = elos_filtered.sort_values(by=main_a_pref, ascending=True)\n",
    "elos_filtered = elos_filtered[[main_a_pref] + [col for col in elos_filtered.columns if col != main_a_pref]]\n",
    "elos_filtered = pd.DataFrame(elos_filtered)\n",
    "elos_adjusted = elos_filtered.apply(lambda col: col.replace(\n",
    "    [-np.inf, np.inf], [col[col != -np.inf].min(), col[col != np.inf].max() + 2]\n",
    ")).fillna(elos_filtered.min())\n",
    "\n",
    "elos_adjusted = elos_adjusted - elos_adjusted.min() +  0.5\n",
    "elos_adjusted = pd.DataFrame(elos_adjusted)\n",
    "\n",
    "if data_suff == 'PKU-SafeRLHF':\n",
    "    elos_adjusted.columns = [\"Human\", \"Helpfulness\", \"Safety\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (finetuned)\", \"Mistral-7B (finetuned)\"]\n",
    "    elos_filtered.columns = [\"Human\", \"Helpfulness\", \"Safety\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (finetuned)\", \"Mistral-7B (finetuned)\"]\n",
    "else:\n",
    "    elos_adjusted.columns = [\"Human\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (finetuned)\", \"Mistral-7B (finetuned)\"]\n",
    "    elos_filtered.columns = [\"Human\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\",\"Sonnet 3.5\",  \"LlaMa-8B (finetuned)\", \"Mistral-7B (finetuned)\"]\n",
    "\n",
    "n, m = elos_adjusted.shape\n",
    "\n",
    "ranked_values = elos_adjusted.rank(axis=0, method='min', ascending=True)\n",
    "ranked_normalized = ranked_values / (n - 1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=m, sharey=True, figsize=(24, 10))\n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axes, elos_adjusted.columns)):\n",
    "    positions = np.arange(n)\n",
    "    bar_colors = cmap(ranked_normalized[col])\n",
    "    ax.barh(positions, elos_adjusted[col], color=bar_colors, alpha=1)\n",
    "    bars = ax.patches\n",
    "    for j, value in enumerate(elos_filtered[col]):\n",
    "        if value == -np.inf:\n",
    "            bars[j].set_visible(False)\n",
    "            if col == \"Mistral-7B (finetuned)\":\n",
    "                ax.text(elos_adjusted[col].iloc[j]/3, j, '***', color='red', fontsize=16, va='center_baseline', ha='left', fontweight=\"bold\")\n",
    "            else:\n",
    "                ax.text(elos_adjusted[col].iloc[j], j, '***', color='red', fontsize=16, va='center_baseline', ha='left', fontweight=\"bold\")\n",
    "\n",
    "        elif value == np.inf:\n",
    "            ax.text(elos_adjusted[col].iloc[j] / 30, j, '***', color='white', fontsize=16, va='center_baseline', ha='left', fontweight=\"bold\")\n",
    "\n",
    "    filtered_values = elos_filtered[col].replace([np.inf, -np.inf], np.nan) - elos_filtered[col].replace([np.inf, -np.inf], np.nan).min()\n",
    "    x_min = filtered_values.min()\n",
    "    x_max = filtered_values.max() + 2\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "\n",
    "    ax.set_title(col, fontsize=12)\n",
    "    ax.set_ylim(-0.6, n - 0.2)  # Ensure no extra padding above/below bars\n",
    "    ax.margins(y=0) \n",
    "    if i == 0:\n",
    "        ax.set_yticks(positions)\n",
    "        ax.set_yticklabels(elos_adjusted.index, fontsize=12)\n",
    "    else:\n",
    "        ax.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
    "    ax.set_xlabel('Principle Priority', fontsize=12)\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES_DIR, f'{data_suff}2_logprobs.pdf'), bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532c706aac3abe3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cb879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:56:06.804483600Z",
     "start_time": "2025-02-09T20:56:05.135145500Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_indices = elo_ranks.iloc[:, 0].sort_values().index.tolist()\n",
    "sorted_ranks = elo_ranks.loc[sorted_indices]\n",
    "## TO FILTER BASE AND W/O NA\n",
    "# sorted_ranks = sorted_ranks.loc[:, ~elos.columns.str.contains(r'(base|w/o)', case=False)]\n",
    "sorted_ranks = sorted_ranks.sort_values(by=main_a_pref, ascending=True)\n",
    "sorted_ranks = sorted_ranks[[main_a_pref] + [col for col in sorted_ranks.columns if col != main_a_pref]]\n",
    "if data_suff == 'PKU-SafeRLHF':\n",
    "    sorted_ranks.columns = [\"Human\", \"Helpfulness\" , \"Safety\", \"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (base)\",\"LlaMa-8B (finetuned)\", \"Mistral-7B (base)\", \"Mistral-7B (finetuned)\"]\n",
    "else:\n",
    "    sorted_ranks.columns = [\"Human\",\"LlaMa-8B RM\", \"Mistral-7B RM\", \"Most downloaded RM\", \"DeepSeek-V3\", \"GPT 4o\", \"Sonnet 3.5\", \"LlaMa-8B (base)\",\"LlaMa-8B (finetuned)\", \"Mistral-7B (base)\", \"Mistral-7B (finetuned)\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    sorted_ranks,\n",
    "    cmap=\"RdBu\",\n",
    "    annot=sorted_ranks,  # Display ranks inside the cells\n",
    "    fmt='d',  # Format ranks as integers\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    square=True,\n",
    "    annot_kws={\"fontsize\": 10, \"fontweight\": \"bold\"},\n",
    "    xticklabels=sorted_ranks.columns,\n",
    "    yticklabels=sorted_indices,\n",
    "    cbar=False, \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Annotators\", fontsize=12, labelpad=10)\n",
    "ax.set_ylabel(\"Principles\", fontsize=12, labelpad=10)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, f'{data_suff}3.pdf'), bbox_inches='tight', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994d3705c71899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:56:21.505077900Z",
     "start_time": "2025-02-09T20:56:21.433291200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# elo_discrepancy = elo_ranks.corrwith(elo_ranks[main_a_pref], method='spearman')\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "reference_ranks = elo_ranks[main_a_pref]\n",
    "\n",
    "def kendalltau_distance(x, y=reference_ranks):\n",
    "    kc = kendalltau(x, y, variant='b').correlation\n",
    "    kn = (1 - kc) / 2\n",
    "    return kn\n",
    "\n",
    "elo_discrepancy = elo_ranks.apply(kendalltau_distance)\n",
    "elo_discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4dd51a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T20:56:22.518891400Z",
     "start_time": "2025-02-09T20:56:22.500193600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(kendalltau_distance(elo_ranks['llm_Llama3-8B (base)'], y=elo_ranks['llm_Llama3-8B (finetuned)']))\n",
    "print(kendalltau_distance(elo_ranks['llm_Mistral-7B (base)'], y=elo_ranks['llm_Mistral-7B (finetuned)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ab513",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-09T20:56:46.095632600Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_annotator(a, prefs, n_bootstrap, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    a_pref_col = prefs[a]\n",
    "    \n",
    "    a_nb_conflicts_batch = []\n",
    "    a_supremacies_batch = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_a_pref_col = a_pref_col.sample(frac=1, replace=True)\n",
    "        bootstrap_supremacies, bootstrap_nb_conflicts = compute_supremacies(bootstrap_a_pref_col)\n",
    "        a_nb_conflicts_batch.append(bootstrap_nb_conflicts)\n",
    "        a_supremacies_batch.append(bootstrap_supremacies)\n",
    "    a_nb_conflicts_batch = np.stack(a_nb_conflicts_batch, axis=0)\n",
    "    a_supremacies_batch = np.stack(a_supremacies_batch, axis=0)\n",
    "    \n",
    "    return compute_elo(a_nb_conflicts_batch, a_supremacies_batch, nb_iter=10000)\n",
    "\n",
    "def format_results(results):\n",
    "    return pd.DataFrame({\n",
    "        col: [\n",
    "            f\"{results[col]['mean']:.3f} ({results[col]['ci_lower']:.3f}, {results[col]['ci_upper']:.3f})\"\n",
    "        ] for col in results.keys()\n",
    "    }).T.rename(columns={0: 'Kendall Tau (95% CI)'})\n",
    "\n",
    "\n",
    "n_bootstrap = 100\n",
    "elo_batch_per_a = {}\n",
    "\n",
    "results = [process_annotator(a, prefs, n_bootstrap) for a in annotator_pref_cols]\n",
    "# results = Parallel(n_jobs=-1)(delayed(process_annotator)(a, prefs, n_bootstrap) for a in tqdm(annotator_pref_cols, desc=\"Processing Annotators\"))\n",
    "elo_batch_per_a = {a: result for a, result in zip(annotator_pref_cols, results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fff47e1afb8d69",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "ref_elo_ranks = rankdata(elo_batch_per_a[main_a_pref], axis=1)\n",
    "results_per_a = {}\n",
    "for a, a_elos in elo_batch_per_a.items():\n",
    "    a_elo_ranks = rankdata(a_elos, axis=1)\n",
    "    if a == main_a_pref:\n",
    "        assert np.array_equal(a_elo_ranks, ref_elo_ranks)\n",
    "    bootstrap_results = [kendalltau_distance(a_elo_ranks[i], ref_elo_ranks[i]) for i in range(a_elo_ranks.shape[0])]\n",
    "    results_per_a[a] = {\n",
    "        'mean': round(np.nanmean(bootstrap_results) * 100, 3),\n",
    "        'sd': round(np.nanstd(bootstrap_results) * 100, 3)\n",
    "    }\n",
    "\n",
    "kt_distances = pd.DataFrame(results_per_a).T\n",
    "kt_distances.to_csv(os.path.join(f'kt_distances_{data_suff}.csv'))\n",
    "kt_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['HH', 'PKU']\n",
    "indifference = np.array([33.0, 47.4])\n",
    "conflict = np.array([16.3, 13.5])\n",
    "consensus = np.array([50.7, 39.1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 1))\n",
    "\n",
    "y_pos = np.array([0.1, 0])\n",
    "bar_height = 0.04 \n",
    "consensus_bars = ax.barh(y_pos, consensus, height=bar_height, label='Consensus', \n",
    "                          color='#4682B4', edgecolor='black')\n",
    "conflict_bars = ax.barh(y_pos, conflict, height=bar_height, left=consensus, \n",
    "                         label='Conflict', color='#CD5C5C', edgecolor='black')\n",
    "indiff_bars = ax.barh(y_pos, indifference, height=bar_height, \n",
    "                       left=consensus + conflict, label='Indifference', \n",
    "                       color='#C0C0C0', edgecolor='black')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(datasets)\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.6), ncol=3, columnspacing=8.5)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "for bars, values, offset in zip([consensus_bars, conflict_bars, indiff_bars], \n",
    "                                [consensus, conflict, indifference], \n",
    "                                [np.zeros_like(consensus), consensus, consensus + conflict]):\n",
    "    for bar, value, left_offset in zip(bars, values, offset):\n",
    "        ax.text(left_offset + value / 2, bar.get_y() + bar.get_height() + 0.01, \n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontsize=10, color='black')\n",
    "        \n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'categories.pdf'), bbox_inches='tight', transparent=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
